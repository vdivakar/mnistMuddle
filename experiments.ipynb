{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cbcb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from mnist import MNIST\n",
    "from sklearn import manifold\n",
    "import numpy as np\n",
    "from time import time\n",
    "import os\n",
    "from matplotlib import offsetbox\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae07651",
   "metadata": {},
   "outputs": [],
   "source": [
    "mndata = MNIST('Data/mnist')\n",
    "train_images, train_labels = mndata.load_training()\n",
    "test_images, test_labels = mndata.load_testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b1c3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = np.array(train_images)\n",
    "y_tr = np.array(train_labels)\n",
    "X_te = np.array(test_images)\n",
    "y_te = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9073f950",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tr = X_tr.reshape([-1, 1,28,28])\n",
    "img_te = X_te.reshape([-1, 1,28,28])\n",
    "print(img_tr.shape)\n",
    "print(img_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f26d95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_te = pd.Series(y_te)\n",
    "df_tr = pd.Series(y_tr)\n",
    "print(df_te.value_counts())\n",
    "print(df_tr.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a0bad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_tr = X_tr.reshape([-1, 28, 28])\n",
    "img_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2919cb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale and visualize the embedding vectors\n",
    "def plot_embedding(X, y, img, title=None):\n",
    "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
    "    X = (X - x_min) / (x_max - x_min)\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    ax = plt.subplot(111)\n",
    "    for i in range(X.shape[0]):\n",
    "        plt.text(X[i, 0], X[i, 1], str(y[i]),\n",
    "                 color=plt.cm.Set1(y[i] / 10.),\n",
    "                 fontdict={'weight': 'bold', 'size': 9})\n",
    "\n",
    "    # if hasattr(offsetbox, 'AnnotationBbox'):\n",
    "    #     # only print thumbnails with matplotlib > 1.0\n",
    "    #     shown_images = np.array([[1., 1.]])  # just something big\n",
    "    #     for i in range(X.shape[0]):\n",
    "    #         dist = np.sum((X[i] - shown_images) ** 2, 1)\n",
    "            # if np.min(dist) < 4e-1:\n",
    "            #     # don't show points that are too close\n",
    "            #     continue\n",
    "            # shown_images = np.r_[shown_images, [X[i]]]\n",
    "            # imagebox = offsetbox.AnnotationBbox(\n",
    "            #     offsetbox.OffsetImage(img[i], cmap=plt.cm.gray_r),\n",
    "            #     X[i])\n",
    "            # ax.add_artist(imagebox)\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    if title is not None:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "X_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d482c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE embedding of the digits dataset\n",
    "print(\"Computing t-SNE embedding\")\n",
    "tsne = manifold.TSNE(n_components=2, init='pca', random_state=0)\n",
    "N = 5000\n",
    "s = time()\n",
    "X_tsne = tsne.fit_transform(X_tr[:N])\n",
    "print(time() - s)\n",
    "\n",
    "plot_embedding(X_tsne, y_tr[:N], img_te.reshape([-1, 28,28,1]),\n",
    "               \"t-SNE embedding of the digits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE embedding of the digits dataset\n",
    "print(\"Computing t-SNE embedding\")\n",
    "tsne = manifold.TSNE(n_components=2, init='pca', random_state=0)\n",
    "N = 700\n",
    "X = X_tr\n",
    "Y = y_tr\n",
    "rand_idx = np.random.randint(len(X), size=N)\n",
    "\n",
    "# print(Y[rand_idx])\n",
    "# print(rand_idx)\n",
    "# print(Y[:500])\n",
    "\n",
    "X_tsne = tsne.fit_transform(X[:N])\n",
    "\n",
    "plot_embedding(X_tsne, Y[:N], img_te.reshape([-1, 28,28,1]),\n",
    "               \"t-SNE embedding of the digits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12a2b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f6ab0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        #Encoder\n",
    "        self.conv1 = nn.Conv2d( 1,  8, 3, stride=2,padding=1) #28x28x1 -> 14x14x8\n",
    "        self.conv2 = nn.Conv2d( 8, 16, 3, stride=2,padding=1) #14x14x8 -> 7x7x16\n",
    "        self.conv3 = nn.Conv2d(16, 32, 3, stride=2) #7x7x16 -> 3x3x32\n",
    "        self.conv4 = nn.Conv2d(32, 32, 3, stride=1) #3x3x32 -> 1x1x32\n",
    "        self.fc1   = nn.Linear(32, 10)\n",
    "        \n",
    "        #Decoder\n",
    "        self.t_fc1   = nn.Linear(10, 32) # [32]\n",
    "        self.t_conv1 = nn.ConvTranspose2d(32, 32, 3, stride=2)\n",
    "        self.t_conv2 = nn.ConvTranspose2d(32, 16, 3, stride=2)\n",
    "        self.t_conv3 = nn.ConvTranspose2d(16,  8, 3, stride=2, padding=1)\n",
    "        self.t_conv4 = nn.ConvTranspose2d(8,   1, 3, stride=2, output_padding=1)\n",
    "    \n",
    "    def encoder(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))        \n",
    "        x = F.relu(self.conv3(x))        \n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x\n",
    "    \n",
    "    def decoder(self, x):\n",
    "        x = F.relu(self.t_fc1(x)) # 10 -> 32\n",
    "        x = x.reshape([-1,32,1,1])\n",
    "        x = F.relu(self.t_conv1(x)) # [m, 1,1,32] -> [m, 3,3,32]\n",
    "        x = F.relu(self.t_conv2(x)) # [m, 3,3,32] -> [m, 7,7,16]\n",
    "        x = F.relu(self.t_conv3(x)) # [m, 7,7,16] -> \n",
    "        x = F.relu(self.t_conv4(x))\n",
    "        return x                    #[m, 1, 28, 28]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        latent_vector    = self.encoder(x)\n",
    "        predicted_output = self.decoder(latent_vector)\n",
    "        return predicted_output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31c6e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a199465",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = torch.randn(5, 1, 28, 28)\n",
    "rout = model.encoder(r)\n",
    "rout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd87b45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.decoder(rout).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d5f13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward(r).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcce8f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tr_tensor = torch.tensor(img_tr)/255.0 #converting from 0-255 to 0.0-1.0\n",
    "y_tr_tensor   = torch.tensor(y_tr)\n",
    "img_te_tensor = torch.tensor(img_te)/255.0 #converting from 0-255 to 0.0-1.0\n",
    "y_te_tensor   = torch.tensor(y_te)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(img_tr_tensor, y_tr_tensor)\n",
    "test_dataset  = torch.utils.data.TensorDataset(img_te_tensor, y_te_tensor)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, num_workers=0)\n",
    "test_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=10, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af3011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fxn  = nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) \n",
    "save_dir  = \"./checkpoints/\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "writer    = SummaryWriter('runs/exp_1')\n",
    "te_writer = SummaryWriter('runs/test_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002aad0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    load_ckpt_num = 19\n",
    "    checkpoint = torch.load(save_dir+\"/ep_{}.ckpt\".format(load_ckpt_num))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    global_epoch = checkpoint['global_epoch']\n",
    "    print(\"loaded global ep: {}\".format(global_epoch))\n",
    "except Exception as e:\n",
    "    print(\"Loading checkpoint failed! - {}\".format(e))\n",
    "    global_epoch = 0\n",
    "    \n",
    "n_epochs = 50\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    for data in train_loader:\n",
    "        images, _ = data\n",
    "        outputs = model.forward(images)\n",
    "        loss = loss_fxn(outputs, images)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss+=(loss.item()*images.size(0))\n",
    "           \n",
    "    if(epoch%5==4):\n",
    "        print(\"Saving Global epoch: {}\".format(global_epoch))\n",
    "        \n",
    "        torch.save({\n",
    "            'global_epoch':global_epoch,\n",
    "            'model_state_dict':model.state_dict(),\n",
    "            'optimizer_state_dict':optimizer.state_dict()\n",
    "        }, save_dir+\"/ep_{}.ckpt\".format(global_epoch))\n",
    "        \n",
    "        writer.add_scalar('train_loss', train_loss/len(train_loader), global_epoch)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            te_loss = 0.0\n",
    "            for te_data in test_loader:\n",
    "                te_imgs, _ = data\n",
    "                te_out = model.forward(te_imgs)\n",
    "                loss_ = loss_fxn(te_out, te_imgs)\n",
    "                te_loss += loss_.item() * te_imgs.size(0)\n",
    "            te_loss = te_loss/len(test_loader)\n",
    "            te_writer.add_scalar('test_loss', te_loss, global_epoch)\n",
    "\n",
    "            print(\"Test Loss: {}\".format(te_loss))\n",
    "    \n",
    "    \n",
    "    global_epoch+=1 \n",
    "    \n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    print(\"Epoch: {} \\t Training Loss: {}\".format(epoch, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d1056e",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(model, images)\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59a0b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.forward(img_te_tensor[18:19])[0]\n",
    "y_pred.shape\n",
    "temp = y_pred.permute(1,2,0).detach().numpy()\n",
    "print(temp.shape)\n",
    "# print(y_pred)\n",
    "# type(y_pred)\n",
    "plt.imshow(temp)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cf5cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_img(tensor):\n",
    "    m = tensor.shape[0]\n",
    "    for i in range(m):\n",
    "        temp = tensor[i].permute(1,2,0).detach().numpy()\n",
    "        plt.imshow(temp)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc202089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a random latent vector.\n",
    "# Generated image in random!!\n",
    "lat = torch.randn([2, 10])\n",
    "print(lat.shape)\n",
    "out = model.decoder(lat)\n",
    "print(out.shape)\n",
    "print(out.shape[0])\n",
    "display_img(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cef95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = model.encoder(img_te_tensor[171:172])\n",
    "l2 = model.encoder(img_te_tensor[1325:1326])\n",
    "l_avg = (l1+l2)/2\n",
    "\n",
    "o1 = model.decoder(l1)\n",
    "o2 = model.decoder(l2)\n",
    "o3 = model.decoder(l_avg)\n",
    "\n",
    "display_img(o1)\n",
    "display_img(o2)\n",
    "display_img(o3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d003db2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ea58d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "l700 = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f051ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE embedding of the digits dataset\n",
    "print(\"Computing t-SNE embedding\")\n",
    "tsne = manifold.TSNE(n_components=2, init='pca', random_state=0)\n",
    "N = 5000\n",
    "s = time()\n",
    "X_tsne = tsne.fit_transform(X_tr[:N])\n",
    "print(time() - s)\n",
    "\n",
    "plot_embedding(X_tsne, y_tr[:N], img_te.reshape([-1, 28,28,1]),\n",
    "               \"t-SNE embedding of the digits\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python382jvsc74a57bd08d34476ea57216c8f5d16219d97d30fdb52288d083256b8a3e2803e14fcb641a",
   "display_name": "Python 3.8.2 64-bit ('pytorch': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}